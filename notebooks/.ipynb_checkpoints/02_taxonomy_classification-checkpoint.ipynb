{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dedd443f-efda-4f76-8838-94784f160035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! We have read the OTU file.\n",
      "Total number of unique OTUs (potential species) discovered: 31263\n",
      "\n",
      "Here are the IDs of the first 10 OTUs:\n",
      "SRR35536287.8657;size=71\n",
      "SRR35536287.9396;size=45\n",
      "SRR35536287.8139;size=42\n",
      "SRR35536287.9307;size=39\n",
      "SRR35536287.17121;size=25\n",
      "SRR35536287.8054;size=17\n",
      "SRR35536287.9986;size=14\n",
      "SRR35536287.10544;size=13\n",
      "SRR35536287.8298;size=13\n",
      "SRR35536287.1129;size=12\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "# Define the path to our OTUs file\n",
    "otus_path = os.path.join(\"..\", \"data\", \"processed\", \"otus.fasta\")\n",
    "\n",
    "# A list to hold the OTU IDs\n",
    "otu_ids = []\n",
    "\n",
    "# Loop through the fasta file and collect the IDs\n",
    "for record in SeqIO.parse(otus_path, \"fasta\"):\n",
    "    otu_ids.append(record.id)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Success! We have read the OTU file.\")\n",
    "print(f\"Total number of unique OTUs (potential species) discovered: {len(otu_ids)}\")\n",
    "print(\"\\nHere are the IDs of the first 10 OTUs:\")\n",
    "for otu_id in otu_ids[:10]:\n",
    "    print(otu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b53bc65b-c8c6-400f-96c2-48910d066e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading PR2 database from: ..\\data\\raw\\pr2_version_5.1.0_SSU_dada2.fasta\n",
      "\n",
      "Successfully parsed the PR2 database!\n",
      "Total reference sequences loaded: 240200\n",
      "\n",
      "Here's a preview of the first 5 entries:\n",
      "                                            sequence    kingdom        phylum  \\\n",
      "0  ATGCTTGTCTCAAAGATTAAGCCATGCATGTCTCAGTATAAGCTTT...  Eukaryota     Alveolata   \n",
      "1  TGATCCTGCCAGTAGTCATATGCTTGTCTCAAAGATTAAGCCATGC...  Eukaryota     Alveolata   \n",
      "2  AAGGGCGACGACAGATGTATCGGTGAATGAGGCTTCGGGCCTGGGG...  Eukaryota  Opisthokonta   \n",
      "3  GTGCCAGCAGCCGCGGTAATTCCAGCTCCAATAGCGCATATTAAAG...  Eukaryota  Opisthokonta   \n",
      "4  TGCATGTCTAAGCACATGCCTTATACGGTGAAGCCGCGAATAGCTC...  Eukaryota  Opisthokonta   \n",
      "\n",
      "         class  \n",
      "0  Dinophyceae  \n",
      "1  Dinophyceae  \n",
      "2   Ascomycota  \n",
      "3   Ascomycota  \n",
      "4     Nematoda  \n",
      "\n",
      "Let's look at the different Kingdoms found:\n",
      "kingdom\n",
      "Eukaryota         223209\n",
      "Bacteria            8028\n",
      "Eukaryota:plas      6765\n",
      "Eukaryota:mito      1890\n",
      "Eukaryota:nucl       148\n",
      "Archaea              131\n",
      "Eukaryota:apic        22\n",
      "Eukaryota:chro         7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "# --- (File finding logic remains the same) ---\n",
    "pr2_path_gz = os.path.join(\"..\", \"data\", \"raw\", \"pr2_version_5.1.0_SSU_dada2.fasta.gz\")\n",
    "pr2_path_fasta = os.path.join(\"..\", \"data\", \"raw\", \"pr2_version_5.1.0_SSU_dada2.fasta\")\n",
    "\n",
    "if os.path.exists(pr2_path_gz):\n",
    "    pr2_path = pr2_path_gz\n",
    "    opener = gzip.open\n",
    "elif os.path.exists(pr2_path_fasta):\n",
    "    pr2_path = pr2_path_fasta\n",
    "    opener = open\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find the PR2 database file!\")\n",
    "\n",
    "print(f\"Reading PR2 database from: {pr2_path}\")\n",
    "\n",
    "# --- (The new, simplified parsing logic) ---\n",
    "pr2_data = []\n",
    "\n",
    "with opener(pr2_path, \"rt\") as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        sequence = str(record.seq)\n",
    "        \n",
    "        # The entire header is the taxonomy string\n",
    "        taxonomy_str = record.id\n",
    "        \n",
    "        # Split the taxonomy string into levels\n",
    "        taxonomy_levels = taxonomy_str.split(';')\n",
    "        \n",
    "        # We will extract the main taxonomic ranks.\n",
    "        # We check the length to avoid errors on incomplete taxonomies.\n",
    "        kingdom = taxonomy_levels[0] if len(taxonomy_levels) > 0 else 'Unknown'\n",
    "        phylum = taxonomy_levels[2] if len(taxonomy_levels) > 2 else 'Unknown'\n",
    "        tax_class = taxonomy_levels[4] if len(taxonomy_levels) > 4 else 'Unknown'\n",
    "        \n",
    "        pr2_data.append([sequence, kingdom, phylum, tax_class])\n",
    "\n",
    "# --- (DataFrame creation remains the same) ---\n",
    "pr2_df = pd.DataFrame(pr2_data, columns=['sequence', 'kingdom', 'phylum', 'class'])\n",
    "\n",
    "print(\"\\nSuccessfully parsed the PR2 database!\")\n",
    "print(f\"Total reference sequences loaded: {len(pr2_df)}\")\n",
    "print(\"\\nHere's a preview of the first 5 entries:\")\n",
    "print(pr2_df.head())\n",
    "\n",
    "print(\"\\nLet's look at the different Kingdoms found:\")\n",
    "print(pr2_df['kingdom'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec1ea635-78fc-4125-929d-70b828d61b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original PR2 count: 240200\n",
      "Filtered Eukaryotic count: 232041\n",
      "Count after removing 'Unknown' labels: 232041\n",
      "\n",
      "Top 20 most abundant classes in the reference database:\n",
      "class\n",
      "Ascomycota           35203\n",
      "Arthropoda           22945\n",
      "Basidiomycota        11833\n",
      "Syndiniales           9003\n",
      "Dinophyceae           8321\n",
      "Embryophyceae         7422\n",
      "Coccidiomorphea       7204\n",
      "Globothalamea         5723\n",
      "Nematoda              5249\n",
      "Gregarinomorphea      5194\n",
      "Kinetoplastea         5127\n",
      "Chlorophyceae         5103\n",
      "Spirotrichea          4205\n",
      "Mucoromycota          4130\n",
      "Mollusca              3517\n",
      "Polycystinea          3453\n",
      "Trebouxiophyceae      3380\n",
      "Opalozoa              3343\n",
      "Oligohymenophorea     3242\n",
      "Bacillariophyceae     3035\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Filter for Eukaryotic sequences ---\n",
    "# We use .str.startswith('Eukaryota') to keep all variations like Eukaryota:mito\n",
    "is_eukaryota = pr2_df['kingdom'].str.startswith('Eukaryota')\n",
    "euk_df = pr2_df[is_eukaryota].copy()\n",
    "\n",
    "print(f\"Original PR2 count: {len(pr2_df)}\")\n",
    "print(f\"Filtered Eukaryotic count: {len(euk_df)}\")\n",
    "\n",
    "# --- 2. Drop rows with 'Unknown' phylum or class ---\n",
    "# These are not useful for training a classifier\n",
    "euk_df = euk_df[(euk_df['phylum'] != 'Unknown') & (euk_df['class'] != 'Unknown')]\n",
    "\n",
    "print(f\"Count after removing 'Unknown' labels: {len(euk_df)}\")\n",
    "\n",
    "# --- 3. Let's see the most common classes we have to work with ---\n",
    "print(\"\\nTop 20 most abundant classes in the reference database:\")\n",
    "print(euk_df['class'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a57b633-699f-4f48-a300-9d7c85523762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing the k-mer function ---\n",
      "Sequence: AGATTACAGATTAC\n",
      "K-mer counts (k=6): {'AGATTA': 2, 'GATTAC': 2, 'ATTACA': 1, 'TTACAG': 1, 'TACAGA': 1, 'ACAGAT': 1, 'CAGATT': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "# --- K-mer generation function ---\n",
    "def get_kmer_features(sequence, k=6):\n",
    "    \"\"\"\n",
    "    Converts a DNA sequence into a dictionary of its k-mer counts.\n",
    "    \n",
    "    Args:\n",
    "        sequence (str): The DNA sequence (e.g., 'AGATTAC').\n",
    "        k (int): The length of the k-mer.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are k-mers and values are their counts.\n",
    "    \"\"\"\n",
    "    kmer_counts = {}\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmer = sequence[i:i+k]\n",
    "        kmer_counts[kmer] = kmer_counts.get(kmer, 0) + 1\n",
    "    return kmer_counts\n",
    "\n",
    "# --- Test the function ---\n",
    "test_sequence = \"AGATTACAGATTAC\"\n",
    "test_kmers = get_kmer_features(test_sequence, k=6)\n",
    "print(\"--- Testing the k-mer function ---\")\n",
    "print(f\"Sequence: {test_sequence}\")\n",
    "print(f\"K-mer counts (k=6): {test_kmers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14aaf06e-8a9d-4b08-8c45-da5c20ff63f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Using the following Top 10 classes for training ---\n",
      "['Ascomycota', 'Arthropoda', 'Basidiomycota', 'Syndiniales', 'Dinophyceae', 'Embryophyceae', 'Coccidiomorphea', 'Globothalamea', 'Nematoda', 'Gregarinomorphea']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kartikesh\\AppData\\Local\\Temp\\ipykernel_11552\\2685057892.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df = top_n_df.groupby('class', group_keys=False).apply(lambda x: x.sample(SAMPLES_PER_CLASS))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Created a balanced dataset with 2000 samples per class ---\n",
      "New class distribution:\n",
      "class\n",
      "Arthropoda          2000\n",
      "Ascomycota          2000\n",
      "Basidiomycota       2000\n",
      "Coccidiomorphea     2000\n",
      "Dinophyceae         2000\n",
      "Embryophyceae       2000\n",
      "Globothalamea       2000\n",
      "Gregarinomorphea    2000\n",
      "Nematoda            2000\n",
      "Syndiniales         2000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Applying k-mer function to all sequences... ---\n",
      "Done applying k-mer function.\n",
      "\n",
      "--- Vectorizing features and labels for ML ---\n",
      "Our feature matrix 'X' has shape: (20000, 11737)\n",
      "Our label vector 'y' has shape: (20000,)\n",
      "\n",
      "Example of a vectorized sequence (first 10 features): \n",
      "[0. 0. 1. 0. 1. 0. 1. 0. 0. 0.]\n",
      "\n",
      "Example of an encoded label: \n",
      "'Arthropoda' is encoded as 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- 1. Select the Top N classes ---\n",
    "N_CLASSES = 10\n",
    "top_n_classes = euk_df['class'].value_counts().nlargest(N_CLASSES).index.tolist()\n",
    "print(f\"--- Using the following Top {N_CLASSES} classes for training ---\\n{top_n_classes}\\n\")\n",
    "\n",
    "# Filter the DataFrame to keep only these top classes\n",
    "top_n_df = euk_df[euk_df['class'].isin(top_n_classes)]\n",
    "\n",
    "# --- 2. Create a balanced subsample ---\n",
    "SAMPLES_PER_CLASS = 2000 # You can adjust this number\n",
    "balanced_df = top_n_df.groupby('class', group_keys=False).apply(lambda x: x.sample(SAMPLES_PER_CLASS))\n",
    "\n",
    "print(f\"--- Created a balanced dataset with {SAMPLES_PER_CLASS} samples per class ---\")\n",
    "print(\"New class distribution:\")\n",
    "print(balanced_df['class'].value_counts())\n",
    "\n",
    "\n",
    "# --- 3. Apply the k-mer function ---\n",
    "# This might take a minute or two to run\n",
    "print(\"\\n--- Applying k-mer function to all sequences... ---\")\n",
    "balanced_df['kmer_counts'] = balanced_df['sequence'].apply(lambda seq: get_kmer_features(seq, k=6))\n",
    "print(\"Done applying k-mer function.\")\n",
    "\n",
    "\n",
    "# --- 4. Vectorize the features and labels ---\n",
    "print(\"\\n--- Vectorizing features and labels for ML ---\")\n",
    "\n",
    "# The DictVectorizer turns our dictionaries of k-mer counts into a numerical matrix\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(balanced_df['kmer_counts'])\n",
    "\n",
    "# The LabelEncoder turns our class names (e.g., 'Ascomycota') into numbers (e.g., 0)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(balanced_df['class'])\n",
    "\n",
    "print(f\"Our feature matrix 'X' has shape: {X.shape}\")\n",
    "print(f\"Our label vector 'y' has shape: {y.shape}\")\n",
    "print(f\"\\nExample of a vectorized sequence (first 10 features): \\n{X[0, :10]}\")\n",
    "print(f\"\\nExample of an encoded label: \\n'{balanced_df['class'].iloc[0]}' is encoded as {y[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7db9d281-3b69-4b22-ba4d-d635a72da33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Split ---\n",
      "Training set size: 16000 samples\n",
      "Testing set size: 4000 samples\n",
      "\n",
      "--- Training the Logistic Regression model... ---\n",
      "Done training.\n",
      "\n",
      "--- Evaluating model performance on the unseen test set... ---\n",
      "\n",
      "Model Accuracy: 0.9960\n",
      "\n",
      "--- Detailed Classification Report ---\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      Arthropoda       0.99      1.00      1.00       400\n",
      "      Ascomycota       0.99      1.00      0.99       400\n",
      "   Basidiomycota       1.00      0.99      0.99       400\n",
      " Coccidiomorphea       1.00      0.99      1.00       400\n",
      "     Dinophyceae       1.00      0.99      0.99       400\n",
      "   Embryophyceae       0.99      0.99      0.99       400\n",
      "   Globothalamea       1.00      1.00      1.00       400\n",
      "Gregarinomorphea       1.00      1.00      1.00       400\n",
      "        Nematoda       1.00      1.00      1.00       400\n",
      "     Syndiniales       0.99      1.00      0.99       400\n",
      "\n",
      "        accuracy                           1.00      4000\n",
      "       macro avg       1.00      1.00      1.00      4000\n",
      "    weighted avg       1.00      1.00      1.00      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --- 1. Split the data into training and testing sets ---\n",
    "# We'll use 80% for training and 20% for testing.\n",
    "# random_state ensures that the split is the same every time we run the code.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"--- Data Split ---\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# --- 2. Initialize and Train the Model ---\n",
    "# We'll use Logistic Regression, a solid and fast classifier.\n",
    "# max_iter is increased to ensure the model has enough time to converge.\n",
    "print(\"\\n--- Training the Logistic Regression model... ---\")\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Done training.\")\n",
    "\n",
    "# --- 3. Evaluate the Model ---\n",
    "print(\"\\n--- Evaluating model performance on the unseen test set... ---\")\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\") # .4f formats the number to 4 decimal places\n",
    "\n",
    "# --- 4. Detailed Performance Report ---\n",
    "# The classification report gives us precision, recall, and f1-score for each class.\n",
    "print(\"\\n--- Detailed Classification Report ---\")\n",
    "# We need to use the label_encoder to get the original class names back for the report\n",
    "class_names = label_encoder.classes_\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbc34888-96ff-4275-84f4-f3e7b64627b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading our discovered OTUs from the deep-sea sample ---\n",
      "Loaded 31263 OTUs.\n",
      "\n",
      "--- Applying k-mer function to our OTUs... ---\n",
      "--- Vectorizing OTU features... ---\n",
      "--- Making predictions on the OTU data... ---\n",
      "--- Generating final report... ---\n",
      "\n",
      "--- Top 20 Most Abundant OTUs and their Predicted Class ---\n",
      "               otu_id  abundance   predicted_class\n",
      "0    SRR35536287.8657         71     Globothalamea\n",
      "1    SRR35536287.9396         45   Coccidiomorphea\n",
      "2    SRR35536287.8139         42     Globothalamea\n",
      "3    SRR35536287.9307         39     Globothalamea\n",
      "4   SRR35536287.17121         25   Coccidiomorphea\n",
      "5    SRR35536287.8054         17     Globothalamea\n",
      "6    SRR35536287.9986         14          Nematoda\n",
      "7   SRR35536287.10544         13        Ascomycota\n",
      "8    SRR35536287.8298         13        Ascomycota\n",
      "9    SRR35536287.1129         12   Coccidiomorphea\n",
      "10  SRR35536287.15462         11     Globothalamea\n",
      "11  SRR35536287.15815         11        Ascomycota\n",
      "12  SRR35536287.11353          9     Globothalamea\n",
      "13  SRR35536287.11649          9     Globothalamea\n",
      "14  SRR35536287.12862          9     Globothalamea\n",
      "15   SRR35536287.9793          9     Globothalamea\n",
      "16  SRR35536287.13526          8     Globothalamea\n",
      "17  SRR35536287.15140          8  Gregarinomorphea\n",
      "18  SRR35536287.19236          8     Globothalamea\n",
      "19  SRR35536287.23577          8        Ascomycota\n",
      "\n",
      "--- Summary of Predicted Classes in the Sample (by abundance) ---\n",
      "predicted_class\n",
      "Ascomycota          8194\n",
      "Globothalamea       7806\n",
      "Coccidiomorphea     5882\n",
      "Gregarinomorphea    3730\n",
      "Basidiomycota       2500\n",
      "Nematoda            1809\n",
      "Arthropoda          1748\n",
      "Dinophyceae          461\n",
      "Syndiniales          185\n",
      "Embryophyceae        180\n",
      "Name: abundance, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load our OTU data from otus.fasta ---\n",
    "print(\"--- Loading our discovered OTUs from the deep-sea sample ---\")\n",
    "otus_path = os.path.join(\"..\", \"data\", \"processed\", \"otus.fasta\")\n",
    "\n",
    "otu_data = []\n",
    "for record in SeqIO.parse(otus_path, \"fasta\"):\n",
    "    # The header looks like 'SRR35536287.8657;size=71'\n",
    "    header_parts = record.id.split(';size=')\n",
    "    otu_id = header_parts[0]\n",
    "    # The size is the number of reads for this OTU, its abundance\n",
    "    abundance = int(header_parts[1])\n",
    "    sequence = str(record.seq)\n",
    "    otu_data.append([otu_id, abundance, sequence])\n",
    "\n",
    "otu_df = pd.DataFrame(otu_data, columns=['otu_id', 'abundance', 'sequence'])\n",
    "print(f\"Loaded {len(otu_df)} OTUs.\")\n",
    "\n",
    "# --- 2. Apply the same k-mer function ---\n",
    "print(\"\\n--- Applying k-mer function to our OTUs... ---\")\n",
    "otu_df['kmer_counts'] = otu_df['sequence'].apply(lambda seq: get_kmer_features(seq, k=6))\n",
    "\n",
    "# --- 3. Vectorize OTUs using the EXISTING trained vectorizer ---\n",
    "# We use .transform() here, NOT .fit_transform(), because the vocabulary is already learned.\n",
    "print(\"--- Vectorizing OTU features... ---\")\n",
    "X_otus = vectorizer.transform(otu_df['kmer_counts'])\n",
    "\n",
    "# --- 4. Make Predictions with our trained model ---\n",
    "print(\"--- Making predictions on the OTU data... ---\")\n",
    "otu_predictions_encoded = model.predict(X_otus)\n",
    "\n",
    "# --- 5. Decode the predictions back to class names ---\n",
    "otu_predictions_decoded = label_encoder.inverse_transform(otu_predictions_encoded)\n",
    "\n",
    "# --- 6. Create the final report ---\n",
    "print(\"--- Generating final report... ---\\n\")\n",
    "otu_df['predicted_class'] = otu_predictions_decoded\n",
    "\n",
    "# Sort the results by abundance to see the most common organisms first\n",
    "final_report = otu_df.sort_values(by='abundance', ascending=False)\n",
    "\n",
    "# Display the Top 20 most abundant classified OTUs\n",
    "print(\"--- Top 20 Most Abundant OTUs and their Predicted Class ---\")\n",
    "print(final_report[['otu_id', 'abundance', 'predicted_class']].head(20))\n",
    "\n",
    "# --- Bonus: Summary of biodiversity found ---\n",
    "print(\"\\n--- Summary of Predicted Classes in the Sample (by abundance) ---\")\n",
    "# This groups by the predicted class and sums up the abundance of all OTUs in that class\n",
    "biodiversity_summary = final_report.groupby('predicted_class')['abundance'].sum().sort_values(ascending=False)\n",
    "print(biodiversity_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "941d63a9-0dc5-431a-b4ae-4b89ae340eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and assets saved to the '..\\models' directory.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- Create a directory to save our model assets ---\n",
    "model_dir = os.path.join(\"..\", \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True) # exist_ok=True prevents an error if the folder already exists\n",
    "\n",
    "# --- Save the four essential objects ---\n",
    "joblib.dump(model, os.path.join(model_dir, 'tax_classifier.joblib'))\n",
    "joblib.dump(vectorizer, os.path.join(model_dir, 'kmer_vectorizer.joblib'))\n",
    "joblib.dump(label_encoder, os.path.join(model_dir, 'label_encoder.joblib'))\n",
    "\n",
    "# We also save the test data for consistent validation\n",
    "np.savez(os.path.join(model_dir, 'test_data.npz'), X_test=X_test, y_test=y_test)\n",
    "\n",
    "print(f\"Model and assets saved to the '{model_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f09daea-f342-4fe0-a997-417f6137f0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
